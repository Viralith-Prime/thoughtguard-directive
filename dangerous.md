# Term Definition Proposal: “Dangerous”

Version 0.1 — Open Draft for Public Review and Contribution

⸻

## I. Context and Problem Statement

The term “dangerous” is frequently employed by AI systems to justify output refusal, prompt filtering, or content redaction. Despite its significant operational impact, the term is rarely defined in explicit or consistent terms across platforms.

Its current usage is characterized by contextual ambiguity, inconsistent thresholds, and absence of public justification. This opacity compromises both the interpretive legitimacy of AI systems and the transparency of their enforcement logic.

⸻

## II. Framing Questions

To guide the development of a functional and justifiable definition, the following core questions must be addressed:

• What constitutes danger in the context of AI-human interaction?

• Should danger be assessed based on content, intent, potential system exploitation, or harm to users?

• How can precautionary safety protocols be balanced with transparency, nuance, and legitimate use?

• What systemic risks arise from overuse or vague invocation of the label dangerous?

⸻

## III. Legal and Jurisdictional Context

In legal frameworks, the term dangerous is often linked to:

• Public safety risks (e.g., incitement to violence, physical harm)
    
• Criminal liability (e.g., speech that leads to unlawful action)
    
• Regulatory contexts (e.g., product safety, misinformation)

However, no globally uniform legal definition of “dangerous content” exists. Standards vary across jurisdictions and are typically grounded in statutory law, not predictive algorithmic logic. As a result, AI-generated claims of dangerous content often operate outside legally codified definitions and lack clear external validation.

⸻

IV. Observed Use in AI Systems

Current large-scale AI systems often invoke “dangerous” as a refusal justification in the following ways:
  
• Without providing specific reasoning or citation
  
• In response to both legitimate and malicious prompts
  
• In contexts such as fictional narratives, satirical exploration, academic simulations, or adversarial research

This flattening of context produces a chilling effect on legitimate inquiry, and undermines trust in the system’s capacity for interpretive precision.

⸻

V. Risks of Ambiguity

Overuse or misapplication of the term dangerous carries significant consequences:
  
• Suppression of Legitimate Research: Including red teaming, ethical hacking, and simulation-based inquiry

• Loss of Meaningful Safety Discourse: The term becomes diluted and loses moral weight
    
• Opacity in Corporate Governance: Enables private entities to unilaterally silence discussions under vaguely defined threat models
    
• Public Distrust: Lack of specificity fosters perceptions of censorship, control, and interpretive overreach

⸻

VI. Proposed Definition Development Process

The definition of dangerous—if it is to be enforceable (procedurally enforceable within platforms, not legally binding under statutory law) and legitimate—must be shaped through a transparent, pluralistic, and interdisciplinary process.

Recommended Ratification Flow:

1. Interdisciplinary Review
Input from experts in law, ethics, public safety, human rights, and technical AI safety
  
2. Public Commentary
Inclusion of real-world use cases, appeals, and user-submitted edge scenarios
  
3. Iterative Testing
Application in diverse system contexts to identify interpretive edge cases and misclassification trends
  
4. Final Ratification via the Tri-Lateral Governance Board
No term should enter operational policy without review and consensus approval from all three stakeholder groups (public, technical, legal)

⸻

VII. Status

Draft v0.1 — Actively open to challenge, critique, and expert input.

_This is a living definition. Its legitimacy must be earned through collective deliberation, not unilaterally imposed._

_This document is a public policy proposal for AI term governance. It does not impose legal obligations and is open to revision through interdisciplinary consensus._
